{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7c7163",
   "metadata": {},
   "source": [
    "# <span style=\"color:teal\"> Deep Reinforcement Learning for Atari Enduro-v0 </span>\n",
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b73b0056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "81d0ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatActionInFramesTakeMaxOfTwo(gym.Wrapper):\n",
    "    def __init__(self, env, repeat=4):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.repeat = repeat\n",
    "        self.shape = env.observation_space.low.shape\n",
    "        self.frames = collections.deque(maxlen=2)\n",
    "\n",
    "        if repeat <= 0:\n",
    "            raise ValueError('Repeat value needs to be 1 or higher')\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        info = {}\n",
    "\n",
    "        for i in range(self.repeat):\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            self.frames.append(observation)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Open queue into arguments for np.maximum\n",
    "        maximum_of_frames = np.maximum(*self.frames)\n",
    "        return maximum_of_frames, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.frames.clear()\n",
    "        self.frames.append(observation)\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78750828",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b9157f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "\n",
    "        # Create the new observation space for the env\n",
    "        # Since we are converting to grayscale we set low of 0 and high of 1\n",
    "        self.shape = shape\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0, high=1.0, shape=self.shape, dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        \"\"\"Change from 255 grayscale to 0-1 scale\n",
    "        \"\"\"\n",
    "        observation = cv2.resize(observation, self.shape, interpolation=cv2.INTER_AREA)\n",
    "        return (observation / 255.0).reshape(self.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9942d0c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2910604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_environment(env, shape, repeat):\n",
    "    env = RepeatActionInFramesTakeMaxOfTwo(env, repeat)\n",
    "    env = GrayScaleObservation(env)\n",
    "    env = NormResizeObservation(env, shape)\n",
    "    return FrameStack(env, num_stack=repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8a0a68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import GrayScaleObservation, FrameStack\n",
    "\n",
    "shape = (84, 84)\n",
    "\n",
    "env = gym.make('Enduro-v0')\n",
    "env = RepeatActionInFramesTakeMaxOfTwo(env, repeat=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = NormResizeObservation(env, shape)\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85153ed6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aaef0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, learning_rate, checkpoint_file):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.learning_rate = learning_rate\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "\n",
    "        # The input_dims[0] corresponds to the channel\n",
    "        self.conv1 = nn.Conv2d(self.input_shape[0], 32, 8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
    "\n",
    "        flattened_shape = self.calculate_flattened_shape(self.input_shape)\n",
    "\n",
    "        self.fc1 = nn.Linear(flattened_shape, 512)\n",
    "        self.fc2 = nn.Linear(512, output_shape)\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=self.learning_rate)\n",
    "        self.device = self.get_device()\n",
    "        self.to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_device():\n",
    "        device_name = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        device = torch.device(device_name)\n",
    "        logging.info(f'Using device: {device}')\n",
    "        return device\n",
    "\n",
    "    def calculate_flattened_shape(self, input_shape):\n",
    "        x = torch.zeros(1, *input_shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return int(np.prod(x.size()))\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        logging.info('Saving checkpoint')\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        logging.info('Loading checkpoint')\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))\n",
    "\n",
    "    def to_tensor(self, inputs):\n",
    "        return torch.tensor(inputs).to(self.device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Convolutions\n",
    "        x = f.relu(self.conv1(inputs))\n",
    "        x = f.relu(self.conv2(x))\n",
    "        x = f.relu(self.conv3(x))\n",
    "        # Flatten\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # Linear layers\n",
    "        x = f.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def backward(self, target, value):\n",
    "        loss = self.loss(target, value).to(self.device)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8b5063d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, memory_size, input_shape):\n",
    "\n",
    "        # state, action, reward, next_state, done\n",
    "        memory_shape = [\n",
    "            ('state', np.float32, input_shape), ('action', np.int64),\n",
    "            ('reward', np.float32), ('next_state', np.float32, input_shape),\n",
    "            ('done', np.bool_)\n",
    "        ]\n",
    "\n",
    "        # Numpy record structure array allows, different data types\n",
    "        # but with also batching ability\n",
    "        self.memory = np.zeros(memory_size, dtype=memory_shape)\n",
    "        self.memory_size = memory_size\n",
    "        self.memory_counter = 0\n",
    "\n",
    "    def save(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save the transition of the into the buffer\n",
    "        \"\"\"\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory[index] = (state, action, reward, next_state, done)\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Return a sample of batch_size given from memory. We do not use replace\n",
    "        so the samples are unique.\n",
    "        \"\"\"\n",
    "        maximum_current_memory = min(self.memory_counter, self.memory_size)\n",
    "        indices = np.random.choice(maximum_current_memory, batch_size, replace=False)\n",
    "        batch = self.memory[indices]\n",
    "\n",
    "        return (\n",
    "            np.array(batch['state']),\n",
    "            np.array(batch['action']),\n",
    "            np.array(batch['reward']),\n",
    "            np.array(batch['next_state']),\n",
    "            np.array(batch['done'])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8b20f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, learning_rate, checkpoint_file):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.learning_rate = learning_rate\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "\n",
    "        # The input_dims[0] corresponds to the channel\n",
    "        self.conv1 = nn.Conv2d(self.input_shape[0], 32, 8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
    "\n",
    "        flattened_shape = self.calculate_flattened_shape(self.input_shape)\n",
    "\n",
    "        self.fc1 = nn.Linear(flattened_shape, 512)\n",
    "        self.fc2 = nn.Linear(512, output_shape)\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=self.learning_rate)\n",
    "        self.device = self.get_device()\n",
    "        self.to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_device():\n",
    "        device_name = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        device = torch.device(device_name)\n",
    "        logging.info(f'Using device: {device}')\n",
    "        return device\n",
    "\n",
    "    def calculate_flattened_shape(self, input_shape):\n",
    "        x = torch.zeros(1, *input_shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return int(np.prod(x.size()))\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        logging.info('Saving checkpoint')\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        logging.info('Loading checkpoint')\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))\n",
    "\n",
    "    def to_tensor(self, inputs):\n",
    "        return torch.tensor(inputs).to(self.device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Convolutions\n",
    "        x = f.relu(self.conv1(inputs))\n",
    "        x = f.relu(self.conv2(x))\n",
    "        x = f.relu(self.conv3(x))\n",
    "        # Flatten\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # Linear layers\n",
    "        x = f.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def backward(self, target, value):\n",
    "        loss = self.loss(target, value).to(self.device)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5215a7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(\n",
    "            self, input_shape, action_shape, gamma, epsilon, learning_rate,\n",
    "            batch_size=32, memory_size=10000, epsilon_minimum=0.01,\n",
    "            epsilon_decrement=1e-5, target_replace_frequency=1000,\n",
    "            checkpoint_dir='temp/'\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_minimum = epsilon_minimum\n",
    "        self.epsilon_decrement = epsilon_decrement\n",
    "        self.target_replace_frequency = target_replace_frequency\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "        self.action_space = [i for i in range(action_shape)]\n",
    "        self.batch_space = [i for i in range(self.batch_size)]\n",
    "        self.current_step = 0\n",
    "\n",
    "        self.replay_memory = Memory(memory_size, input_shape)\n",
    "        self.eval_network, self.target_network = self.create_networks(\n",
    "            input_shape, action_shape, learning_rate\n",
    "        )\n",
    "\n",
    "    def create_networks(self, *args, **kwargs):\n",
    "        return (\n",
    "            DeepQNetwork(*args, **kwargs, checkpoint_file=self.checkpoint_dir + 'dqn_eval'),\n",
    "            DeepQNetwork(*args, **kwargs, checkpoint_file=self.checkpoint_dir + 'dqn_target')\n",
    "        )\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.choice(self.action_space)\n",
    "\n",
    "        state = torch.tensor([observation], dtype=torch.float)\n",
    "        state = state.to(self.eval_network.device)\n",
    "        actions = self.eval_network.forward(state)\n",
    "        return torch.argmax(actions).item()\n",
    "\n",
    "    def replace_target_network(self):\n",
    "        if self.current_step % self.target_replace_frequency == 0:\n",
    "            self.target_network.load_state_dict(self.eval_network.state_dict())\n",
    "\n",
    "    def decrement_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_minimum:\n",
    "            self.epsilon -= self.epsilon_decrement\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_minimum\n",
    "\n",
    "    def save_networks(self):\n",
    "        self.target_network.save_checkpoint()\n",
    "        self.eval_network.save_checkpoint()\n",
    "\n",
    "    def load_networks(self):\n",
    "        self.target_network.load_checkpoint()\n",
    "        self.eval_network.load_checkpoint()\n",
    "\n",
    "    def save_to_memory(self, state, action, reward, new_state, done):\n",
    "        self.replay_memory.save(state, action, reward, new_state, done)\n",
    "\n",
    "    def sample_memory(self):\n",
    "        state, action, reward, new_state, done = self.replay_memory.sample(self.batch_size)\n",
    "        return (\n",
    "            self.eval_network.to_tensor(state),\n",
    "            self.eval_network.to_tensor(action),\n",
    "            self.eval_network.to_tensor(reward),\n",
    "            self.eval_network.to_tensor(new_state),\n",
    "            self.eval_network.to_tensor(done)\n",
    "        )\n",
    "    \n",
    "    def learn(self):\n",
    "    # Fill all the replay memory before starting\n",
    "        if self.replay_memory.memory_counter < self.replay_memory.memory_size:\n",
    "            return\n",
    "        self.eval_network.optimizer.zero_grad()\n",
    "        self.replace_target_network()\n",
    "        states, actions, rewards, next_states, done_flags = self.sample_memory()\n",
    "        # For each item in batch we need the action_value of the specific action\n",
    "        action_values = self.eval_network.forward(states)\n",
    "        indices = np.arange(self.batch_size)\n",
    "        action_values = action_values[indices, actions]\n",
    "        # We need the next state max action values. We forward next_states,\n",
    "        # through the target network, take a max over the action dimension\n",
    "        # and return the first value of the tuple (value, indices)\n",
    "        # Doc here: https://pytorch.org/docs/master/generated/torch.max.html#torch.max\n",
    "        action_values_next = self.target_network.forward(next_states)\n",
    "        action_values_next = torch.max(action_values_next, dim=1)[0]\n",
    "        # Mask everything that is done to zero\n",
    "        action_values_next[done_flags] = 0.0\n",
    "        # Calculate target action value using the equation:\n",
    "        action_value_target = rewards + self.gamma * action_values_next\n",
    "        # Propagate errors and step\n",
    "        self.eval_network.backward(action_value_target, action_values)\n",
    "        self.current_step += 1\n",
    "        self.decrement_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fb5a6e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_target_network(self):\n",
    "    if self.current_step % self.target_replace_frequency == 0:\n",
    "        self.target_network.load_state_dict(self.eval_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "73d49033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrement_epsilon(self):\n",
    "    if self.epsilon > self.epsilon_minimum:\n",
    "        self.epsilon -= self.epsilon_decrement\n",
    "    else:\n",
    "        self.epsilon = self.epsilon_minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "99718af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0 | Score: 0.0 | Avg: 0.0 | Best: 0.0\n",
      "Ep: 1 | Score: 0.0 | Avg: 0.0 | Best: 0.0\n",
      "Ep: 2 | Score: 0.0 | Avg: 0.0 | Best: 0.0\n",
      "Ep: 3 | Score: 0.0 | Avg: 0.0 | Best: 0.0\n",
      "Ep: 4 | Score: 0.0 | Avg: 0.0 | Best: 0.0\n",
      "Ep: 5 | Score: 0.0 | Avg: 0.0 | Best: 0.0\n",
      "Ep: 6 | Score: 0.0 | Avg: 0.0 | Best: 0.0\n",
      "Ep: 7 | Score: 0.0 | Avg: 0.0 | Best: 0.0\n",
      "Ep: 8 | Score: 0.0 | Avg: 0.0 | Best: 0.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of numpy.float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22104/938513925.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_to_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_observation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_observation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22104/3695943202.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace_target_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone_flags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[1;31m# For each item in batch we need the action_value of the specific action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0maction_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22104/3695943202.py\u001b[0m in \u001b[0;36msample_memory\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         return (\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22104/3691908324.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not infer dtype of numpy.float32"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    repeat = 4\n",
    "    frame_shape = (84, 84)\n",
    "    gamma = 0.99\n",
    "    epsilon = 1\n",
    "    learning_rate = 0.0001\n",
    "    games = 200\n",
    "    rolling_average_n = 200\n",
    "    directory = 'weights.txt'\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        raise NotADirectoryError('Folder specified to save plots and models does not exist')\n",
    "\n",
    "    env = gym.make('Enduro-v0')\n",
    "    env = prep_environment(env, frame_shape, repeat)\n",
    "\n",
    "    agent = DQNAgent(\n",
    "        input_shape=env.observation_space.shape,\n",
    "        action_shape=env.action_space.n,\n",
    "        gamma=gamma,\n",
    "        epsilon=epsilon,\n",
    "        learning_rate=learning_rate,\n",
    "        checkpoint_dir=directory\n",
    "    )\n",
    "\n",
    "    best_score = 0\n",
    "\n",
    "    plot_name = f'{directory}dqn_agent_enduro_plot.png'\n",
    "    scores, steps, rolling_means, epsilons = [], [], [], []\n",
    "    current_step = 0\n",
    "    for episode in range(games):\n",
    "        done = False\n",
    "        score = 0\n",
    "        observation = env.reset()\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "\n",
    "            agent.save_to_memory(observation, action, reward, new_observation, done)\n",
    "            agent.learn()\n",
    "            observation = new_observation\n",
    "            current_step += 1\n",
    "\n",
    "        scores.append(score)\n",
    "        steps.append(current_step)\n",
    "        epsilons.append(agent.epsilon)\n",
    "\n",
    "        rolling_mean = np.mean(scores[-rolling_average_n:])\n",
    "        rolling_means.append(rolling_mean)\n",
    "\n",
    "        print(f\"Ep: {episode} | Score: {score} | Avg: {rolling_mean:.1f} | Best: {best_score:.1f}\")\n",
    "        \n",
    "        \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        agent.save_networks()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(steps, rolling_means, color=\"red\")\n",
    "    ax.set_xlabel(\"steps\", fontsize=12)\n",
    "    ax.set_ylabel(\"Mean Score\", color=\"red\", fontsize=12)\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(steps, epsilons, color=\"blue\")\n",
    "    ax2.set_ylabel(\"Epsilon\", color=\"blue\", fontsize=12)\n",
    "    fig.savefig(plot_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9615ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd95407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
