{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:teal\"> Deep Reinforcement Learning for Atari Enduro-v0 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, concatenate, Permute\n",
    "from keras.layers import Input, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.activations import relu, linear\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### RoadRunner Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Enduro-v0')\n",
    "\n",
    "plt.imshow(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. *Number of possible action*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Possible actoin is : 9\n"
     ]
    }
   ],
   "source": [
    "nb_actions = env.action_space.n\n",
    "print('Total number of Possible actoin is :', nb_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. *Taking stack of 4 consecutive frames*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape is : (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "frame_shape = (84, 84)\n",
    "window_length = 4\n",
    "input_shape = (window_length,) + frame_shape\n",
    "print('Input Shape is :', input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "### Defining class for pre-processing the game_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameProcess(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        img = Image.fromarray(observation)\n",
    "        img = np.array(img.resize(frame_shape).convert('L'))\n",
    "        return img.astype('uint8')  \n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        Processed_batch = batch.astype('float32') / 255.\n",
    "        return Processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## DeepMind Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " permute (Permute)           (None, 84, 84, 4)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 20, 20, 32)        8224      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 20, 20, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 9, 9, 64)          32832     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 9, 9, 64)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1606144   \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 9)                 4617      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 9)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,688,745\n",
      "Trainable params: 1,688,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "model.add(Conv2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Configuring the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. *Allocating memory for experience replay*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000000, window_length=window_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.* Policy: Epsilon Greedy Exploration*\n",
    "<span style=\"color:teal\">*Gradually exploration will be decreased*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. *Compiling DQN Agent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory, processor=GameProcess(),\n",
    "               nb_steps_warmup=50000, gamma=.99, target_model_update=10000, train_interval=4, delta_clip=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <span style=\"color:teal\"> Training the model </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">**-  -  Caution   -  -**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 200000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   46/10000 [..............................] - ETA: 33s - reward: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lauri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 33s 3ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - lives: 0.000 - episode_frame_number: 6086.872 - frame_number: 15075.134\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 33s 3ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - lives: 0.000 - episode_frame_number: 6503.242 - frame_number: 45016.189\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 33s 3ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - lives: 0.000 - episode_frame_number: 6858.149 - frame_number: 74879.806\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 34s 3ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - lives: 0.000 - episode_frame_number: 7208.056 - frame_number: 104923.461\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 33s 3ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - lives: 0.000 - episode_frame_number: 6099.139 - frame_number: 134842.153\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 264s 26ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 0.064 - mean_q: 0.076 - mean_eps: 0.951 - lives: 0.000 - episode_frame_number: 6477.642 - frame_number: 164853.169\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 0.064 - mean_q: 0.074 - mean_eps: 0.942 - lives: 0.000 - episode_frame_number: 6896.166 - frame_number: 194926.835\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 0.066 - mean_q: 0.076 - mean_eps: 0.933 - lives: 0.000 - episode_frame_number: 7092.977 - frame_number: 224906.583\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 0.066 - mean_q: 0.075 - mean_eps: 0.924 - lives: 0.000 - episode_frame_number: 6135.596 - frame_number: 255031.391\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 0.066 - mean_q: 0.075 - mean_eps: 0.915 - lives: 0.000 - episode_frame_number: 6526.300 - frame_number: 285026.652\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 0.065 - mean_q: 0.075 - mean_eps: 0.906 - lives: 0.000 - episode_frame_number: 6900.775 - frame_number: 315022.989\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 269s 27ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 0.066 - mean_q: 0.075 - mean_eps: 0.897 - lives: 0.000 - episode_frame_number: 7066.319 - frame_number: 344983.452\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 0.066 - mean_q: 0.075 - mean_eps: 0.888 - lives: 0.000 - episode_frame_number: 6156.780 - frame_number: 374999.020\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 0.065 - mean_q: 0.074 - mean_eps: 0.879 - lives: 0.000 - episode_frame_number: 6548.854 - frame_number: 405048.899\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 271s 27ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 0.066 - mean_q: 0.075 - mean_eps: 0.870 - lives: 0.000 - episode_frame_number: 6898.311 - frame_number: 435025.543\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0000e+00\n",
      "3 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 0.067 - mean_q: 0.077 - mean_eps: 0.861 - lives: 0.000 - episode_frame_number: 6988.087 - frame_number: 464923.549\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 0.067 - mean_q: 0.076 - mean_eps: 0.852 - lives: 0.000 - episode_frame_number: 6170.571 - frame_number: 494903.346\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 1158s 116ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 0.066 - mean_q: 0.076 - mean_eps: 0.843 - lives: 0.000 - episode_frame_number: 6574.025 - frame_number: 524858.108\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 271s 27ms/step - reward: 0.0000e+00\n",
      "2 episodes - episode_reward: 0.000 [0.000, 0.000] - loss: 0.000 - mae: 0.066 - mean_q: 0.076 - mean_eps: 0.834 - lives: 0.000 - episode_frame_number: 6912.780 - frame_number: 554885.285\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: 0.0000e+00\n",
      "done, took 5077.475 seconds\n"
     ]
    }
   ],
   "source": [
    "history = dqn.fit(env, nb_steps=200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">**-  -  -  Safe  -  -  -**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('../dqn_atari_Enduro.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <span style=\"color:teal\"> Testing the model </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33284/2232909420.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Enduro-v5'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "dqn.test(gym.make('Enduro-v0',render_mode='human'), nb_episodes=1, visualize=True)\n",
    "\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Trained on: Intel® Xeon® Processor E5, 2.40 GHz, Nvidia Quadro K4200**\n",
    "***\n",
    "<span style=\"color:teal\"> [Bhartendu Thakur](https://github.com/matrixBT), Machine Learning & Computing</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
